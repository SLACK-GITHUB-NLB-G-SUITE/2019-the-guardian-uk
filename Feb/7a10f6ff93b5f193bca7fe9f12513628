Counting the bodies in conflicts is a necessary, confusing and too often sordid business. Body counts are necessary for obvious reasons. Numbers supply a moral reference point. They tell us about the scale of a conflict as well as if civilians were targeted and how. They provide evidence for different kinds of human rights advocacy in an international setting, and assist in setting policy for emergency assistance. Where counting the bodies becomes sordid, however, is when the process becomes political and weaponised for a purpose; that is, when it is in hock to competing agendas. Official UN statistics put the death toll in Yemen as of March 2018 at 6,592, with 10,470 people injured. International organisations say the number of deaths is somewhere between 56,000 and 80,000 deaths. Each side accuses the other of reducing or inflating numbers to suit their own agenda. The problem – as the current conflict in Yemen is demonstrating, and the Iraq war showed before – is that counting the cost of war is a far from exact science. Not least when it comes to the often fraught reckoning of a figure that can include both direct casualties of violence and those who have died from conflict’s secondary effects, such as limited access to health care. From Iraq to Darfur, the Democratic Republic of the Congo, and now Yemen, accounting for mortality in conflict has become an evidential battlefield. The issue was dramatised again this week by the clumsy and one-sided intervention of Graham Jones, the Labour chair of the Commons committees on arms export controls (CAEC), who unintentionally underlined the difficulty of estimating mortality in conflict – in this case, civilian deaths from Saudi-coalition airstrikes. Accusing international NGOs of dishonesty in their reporting, Jones promptly demonstrated his own agenda by seeking to paint Iran, backer of the rebel Houthis, as the principal aggressor. “It’s disgraceful how NGOs and loony leftwing organisations have refused to back the UN’s unanimous position,” said Jones, provoking fury from aid groups. “We desperately need peace in Yemen, not fantasy answers made in safe European homes.” And while Jones has complacently suggested that he feels far more comfortable with lower casualty tallies provided by “generals and the Ministry of Defence,” it is significant that, across the Atlantic, the Pentagon was sufficiently concerned about the issue to launch a major exercise aimed at understanding why its own accounting of civilian deaths is so much lower than that of respected watch groups. But if Jones’ comments appear grotesquely one-sided – gifting the Saudi-led coalition apparent impunity for its role in the war’s human toll – they do, at least, underscore a wider problem in Yemen: the huge disparity in estimates of the number of victims of the war. As of late last year, those estimates ranged from about 10,000 dead to almost six times that number. Few believe the lower estimate, while for it’s part Save the Children has claimed that 85,000 children under the age of five alone “may” have died from starvation during three years of conflict. So who is getting it wrong? The reality, as anyone who has studied the figures for malnutrition and mortality in Yemen, is that – like Iraq – the numbers offered for public consumption encompass a vast spread of estimates. If Jones has a point, it is that historically some NGOs have undoubtedly been guilty of a process of misery inflation. This is because they have sometimes sought to dramatise crises with the aim of highlighting their own interventions. The real problem is not simply bad faith, but the paucity of tools applied to estimate mortality, not least the lack of baselines for comparison that has in the past led to guesstimates described, in at least one case, as no better than “statistical anarchy”. Lack of continuous and accurate census and health data in numerous recent conflicts has made it difficult to effectively assess real increases in mortality. That has left both UN agencies and NGOs to rely on far more fallible systems, to draw on methodologies open to error, bias and manipulation. The use of randomised household surveys designed to establish the number of deaths in conflict due to secondary causes is a case in point. Some issues are painfully obvious: locations that cannot be visited because of security concerns leave holes in the data or, as in Yemen, require institutions allied with one of the warring parties to fill in the gaps. This leaves room to inject deliberate distortion. Political considerations governing organisations operating in the field – not least UN agencies – can lead to uncomfortable trade-offs, not least over access. Perhaps most difficult of all is unintended sampling biases. This was a contentious issue in the Lancet’s estimate of deaths in excess of normal fatality rates in Iraq. The journal was accused of failing to account for “main-street bias” – relying too heavily on locations where, critics said, more violence took place. This was denied by the authors. None of this is news to those who, having made it their business to study mortality estimates, have long warned of their vulnerability to misinterpretation and manipulation. As long ago as 2005, Francesco Checchi and Les Roberts, who have studied the issue, warned of many of these problems in a paper for the Humanitarian Practice Network. Writing about humanitarian emergencies, both natural and – like conflict – manmade, they warned: “Mortality data are extremely liable to misinterpretation and manipulation.”. They added that “many would argue that recent years have seen the increasing use of relief as a tool for applying international political pressure or improving the image of occupying powers among the local population”. Ironically, perhaps, Jones’ intervention is an urgent and overdue reminder that body counts need to be professional and unbiased, and that all actors – NGOs, UN agencies and influential MPs – must be transparent not only about what is known about the death toll in conflicts, but also about the limitations of their knowledge. The alternative, as Checchi and Roberts argue, is a grim prospect, whether the problem is underestimation or its flipside. “The consequences of bad science can be counted in human lives when, on the basis of incorrect findings, agencies or donors decide to scale down or abandon life-saving activities, or allocate them improperly.”