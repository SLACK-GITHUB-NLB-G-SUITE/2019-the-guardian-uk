Facial recognition technology will not be deployed at the King’s Cross development in the future, following a backlash prompted by the site owner’s admission last month that the software had been used in its CCTV systems. The developer behind the prestigious central London site said the surveillance software had been used between May 2016 and March 2018 in two cameras on a busy pedestrian street running through its heart. It said it had abandoned plans for a wider deployment across the 67-acre, 50-building site and had “no plans to reintroduce any form of facial recognition technology at the King’s Cross Estate”. The site became embroiled in the debate about the ethics of facial recognition three weeks ago after releasing a short statement saying its cameras “use a number of detection and tracking methods, including facial recognition”. That made it one of the first landowners to acknowledge it was deploying the software, described by human rights groups as authoritarian, partly because it captures and analyses images of people without their consent. Despite a series of questions about the scheme from the Information Commissioner’s Office, the London mayor, Sadiq Khan, and expert academics, King’s Cross had offered no further comment until Monday. Daragh Murray, a senior lecturer in human rights law at the University of Essex, said it was reassuring that the developer had abandoned the technology “in the light of the public outcry”, but he added: “This also serves to highlight the lack of transparency. We still don’t really know what they were doing with the technology and how they were able to get away with using it for so long without the public knowing.” Cameras using the software have been used by police forces and landlords to scan faces in streets, shopping centres, football stadiums and events such as the Notting Hill carnival. Images harvested can then be compared to a database of persons of interest. The use of facial recognition software by police in south Wales is being challenged in the courts by an office worker in Cardiff in a test case backed by the Liberty campaign group. The result is keenly anticipated by regulators and across the industry. A trial of facial recognition software last year at the Trafford Centre in Greater Manchester was halted because visitors were being checked against a set of 30 suspects and missing persons provided by the police. Concerns were raised that the trial monitored too many ordinary people. The technology’s accuracy is also unproven. Researchers at the University of Essex were invited by the Metropolitan police to study the force’s trials of its facial recognition software and concluded that it could be sure the right person had been identified in only 19% of the 42 cases studied. On Monday, King’s Cross argued that the use of facial recognition software was more limited than its initial statement may have suggested. The two cameras, operational between 2016 and 2018, were sited on the busy King’s Boulevard, used by pedestrians and cyclists. They were used to help the police in detecting crime and “ultimately to help ensure public safety”, its statement said. Any data processed was regularly deleted and the final deletion took place in March 2018, it added. However, the developer appeared to hold out the possibility of using the technology in the future, saying “we note the broad debate now under way” and adding that its pledge not to reintroduce facial recognition technology applied “in the meantime”. King’s Cross is owned by a consortium of property developer Argent, Hermes Investment Management on behalf of BT Pensioners, and the Australian pension scheme AustralianSuper. The site, mostly to the north of the mainline railway station, includes the headquarters of Google and the Central Saint Martins art school. The Guardian is sited on the fringe of the development.