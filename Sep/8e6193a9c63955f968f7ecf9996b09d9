Tech companies should stop behaving as though everything that is not illegal is acceptable, says Microsoft’s second-in-command. Instead, they should focus on defining – and living by – the standards that they would like to see in regulation, before it gets forced on them anyway. For some of the most potentially dangerous new technologies, such as facial recognition, that could mean voluntarily refusing to sell them to certain countries, for certain uses, or even agreeing to a moratorium altogether, said Brad Smith, the president and chief legal officer of the world’s most valuable publicly-traded company. Speaking to the Guardian before the launch of his new book, Tools and Weapons, Smith said that if technology firms wanted to be proud of how they changed the world for the better, they must take more responsibility for the ways they have made it worse. “When you think about all of the issues that people worry about in the world today and what they spend their time arguing about, it’s often issues like trade, immigration, nationalism, globalisation,” Smith said. “But all of these sit on top of a foundation. And the foundation is really being driven by technology.” Smith cites facial recognition software as one of the core battles still to be fought. Microsoft has already voluntarily imposed limits on how and to whom it sells the technology, he says, and he encourages other firms to do the same. “We’re not willing to allow our facial recognition services to be used for mass surveillance … so if we thought that the US government or that [US immigration agency] Ice was going to deploy facial recognition for mass surveillance, we would object to that and I don’t think we would do it. “If an agency in any government wants to deploy facial recognition in a manner that we believe will result in unfair bias and discrimination, that’s something that we won’t do.” The conversation around the technology has advanced rapidly, even in the gap between Smith finishing the manuscript and the book hitting shelves. “I’ve never seen an issue evolve as quickly as facial recognition has,” Smith said. “And I think there’s a lot of food for thought in that. I say that in part because we feel it deeply ourselves.” “It was just 13 months ago where we published a blogpost where we said: ‘This is technology that is going to need to be regulated.’ The reaction in Silicon Valley was like: ‘What is wrong with you people at Microsoft, why are you doing this?’ And then look at where we are now: we just saw the city of San Francisco, next door to Silicon Valley, ban public sector use of facial recognition.” But, although Smith calls for regulation generally, he argues against an all-out ban. “The first question one should ask is why, why is this happening? It is a technology that can be deployed in, literally, an Orwellian fashion. But I think whenever you want to ban a technology, you also have to ask, well, what are the potentials for it to do good as well? And so then the question is how do you strike the balance? I don’t think that you strike that balance by banning all use. You strike that balance by banning the harmful use. “Now, I respect the people who call for a moratorium, because implicit, or maybe explicit, in the word moratorium is the notion that this will not last forever. I worry about moratoriums nonetheless, just because it can make it more difficult to learn how to use the technology well if you can’t put the technology to use. But I respect that point of view.” A month after Smith spoke to the Guardian, 76 protesters were arrested picketing the Manhattan Microsoft store with signs reading “No Business With Ice”, as part of the Close The Camps movement which demands an end to detention centres for migrants. The protest organisers told CNN that “it is our responsibility to hold US corporations accountable … in the racist torture and trauma that is being inflicted. We have the power to end this cruelty.” In the book, Smith describes Microsoft’s contract with Ice as a surprise to many in the company. A blogpost celebrating the deal, posted before the Trump administration started to set up detention camps on the US border, was rediscovered by activists in June. “A marketing statement made several months earlier now looked a good deal different,” Smith writes. “As we dug to the bottom of the matter, we learned that the contract wasn’t being used for facial recognition at all. Nor, thank goodness, was Microsoft working on any projects to separate children from their families at the border.” Smith says he distinguishes the sale of technology such as facial recognition from the more mundane productivity software that Microsoft does supply to Ice, and to other controversial state actors around the world. “One must assume responsibility for how your products are used in specific, well-defined, clearcut and relatively narrow ways. I think it’s a different situation to try to deny access to all technology because you disagree with what people are doing in their lives or with their businesses, or as a government. So to say, ‘I’m not going to let you use Microsoft Word or our search engine on the internet’, I think that’s a very different dynamic.”